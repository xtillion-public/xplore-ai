{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "57bjio2znjfg2ug2oxpu",
   "authorId": "4377333429049",
   "authorName": "MBARILLAS@XTILLION.COM",
   "authorEmail": "mbarillas@xtillion.com",
   "sessionId": "d2e5df43-2ff6-4438-808d-aa9e6b345969",
   "lastEditTime": 1759955504222
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d57c6de-b65d-4a9a-9a0e-5c8e1c6552ee",
   "metadata": {
    "name": "intro",
    "collapsed": false
   },
   "source": "# Demo: LLM-Powered Analytics with Snowflake Cortex\n\n\n#### In this notebook, we will analyze Mayag√ºez restaurants data scraped from Yelp. We will:\n1) Read a public Parquet file from an AWS S3 bucket, containing the scraped data \n2) Ingest the data into Snowflake \n3) Perform analytics using Snowflake Cortex AI (Meta Llama 3 8B model)\n#####\n\nNote that this is a small dataset and we will be using an XS warehouse (i.e., single node). For your projects, you can leverage Snowflake's distributed compute capabilities for larger datasets and use a larger warehouse if needed (e.g., an X-Large warehouse which has 16 nodes and 128 CPU cores). Of course, a larger warehouse will consume more credits so we encourage you to be mindful of credit usage! At the same time, do not panic, remember you are getting a $400 free trial...\n\nAlso note that this notebook must be executed inside Snowflake. Running the code locally won't work. "
  },
  {
   "cell_type": "code",
   "id": "4142a35d-a686-42b6-bd72-413813562e76",
   "metadata": {
    "language": "python",
    "name": "set_up",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "import streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\nimage_data = session.file.get_stream(\"@DEMO_ASSETS/demo_pipeline.png\",decompress=False).read()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7765e9ce-3c67-482d-ab89-168a19e6b61d",
   "metadata": {
    "language": "python",
    "name": "process_photo",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.image(image_data)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4da7a565-879f-4117-a274-756898aa9f4e",
   "metadata": {
    "name": "step_1",
    "collapsed": false
   },
   "source": "# Step 1: Read Parquet Data From S3 \n\nThis is a public S3 bucket that we've made available for today's demos. In your projects, you can create your own S3 buckets to store your project's data and read/analyze it within Snowflake."
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "read_s3"
   },
   "source": "-- Not good practice! But fine for today's demo...\nUSE ROLE ACCOUNTADMIN;\nGRANT DATABASE ROLE SNOWFLAKE.CORTEX_USER TO ROLE ACCOUNTADMIN;\n\n-- Select a warehouse (has to be created already, but this is super easy!)\nUSE WAREHOUSE COMPUTE_WH;\n\n-- Ensure the demo database exists, if not create it\nCREATE DATABASE IF NOT EXISTS UPRM_BIG_DATA;\nUSE DATABASE UPRM_BIG_DATA;\n\n-- Create a schema for the demo\nCREATE SCHEMA IF NOT EXISTS AI_ANALYTICS_DEMO;\nUSE SCHEMA AI_ANALYTICS_DEMO;\n\nCREATE OR REPLACE STAGE yelp_public_stage\n  URL = 's3://uprm-2025-demo-yelp';\n  \nSELECT 'Successfully created Snowflake Stage from S3' AS note;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "06c36a95-fc1d-423c-b1bb-4e13671d7575",
   "metadata": {
    "language": "sql",
    "name": "check_stage",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Verify that you see a csv in the Snowflake stage\n-- note that Parquet achieves ~2x compression compared to CSV\nLIST @yelp_public_stage PATTERN='^(?!.*json).*';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6620df47-96b3-4c30-8e01-275ea64ade59",
   "metadata": {
    "name": "step_2",
    "collapsed": false
   },
   "source": "# Step 2: Load data into Snowflake Table\n\nIn Step 1 we loaded raw files into a Snowflake Stage, but they do not exist in a table yet. Here we create a table and insert the data there."
  },
  {
   "cell_type": "code",
   "id": "999f5f61-996f-41e5-81a6-18ab76773f1c",
   "metadata": {
    "language": "sql",
    "name": "insert_data"
   },
   "outputs": [],
   "source": "-------------------------------------------------\n-- 2. DDL for Yelp Table\n-------------------------------------------------\nCREATE OR REPLACE TABLE yelp_reviews (\n    business_url STRING,\n    business_name STRING,\n    average_rating FLOAT,\n    total_reviews STRING,\n    price_range STRING,\n    business_address STRING,\n    contact_number STRING,\n    latest_reviewer_name STRING,\n    review_avatar_url STRING,\n    review_id STRING,\n    latest_reviewer_location STRING,\n    latest_reviewer_rating FLOAT,\n    review_date DATE,\n    review_text STRING,\n    helpful_count INT,\n    thanks_count INT,\n    love_this_count INT,\n    oh_no_count INT,\n    response_author_name STRING,\n    response_date STRING,\n    response_content STRING\n);\n\n-------------------------------------------------\n-- 3. Load data into Yelp Table\n-------------------------------------------------\nCOPY INTO yelp_reviews\nFROM @yelp_public_stage/dataset_yelp-reviews-scraper_2025-10-06_15-10-02-775.parquet\nFILE_FORMAT = (TYPE = PARQUET)\nMATCH_BY_COLUMN_NAME = CASE_INSENSITIVE\nON_ERROR = 'CONTINUE';\n\n-------------------------------------------------\n-- 4. Setup completion check\n-------------------------------------------------\nSELECT 'Successfully inserted data into Snowflake table' AS note;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a9e91ab0-26f0-4bf8-95c2-0e7bfb9f6ba2",
   "metadata": {
    "language": "sql",
    "name": "check_table"
   },
   "outputs": [],
   "source": "SELECT * FROM yelp_reviews;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f45bcb14-9783-4a86-b3e2-d8876d6ac975",
   "metadata": {
    "name": "step_3",
    "collapsed": false
   },
   "source": "# Step 3: Analyze sentiment with LLM using a SQL query!\n\nYes, it's as simple as that! Snowflake takes care of the LLM infrastructure management and provides a SQL API to do LLM queries via `SNOWFLAKE.CORTEX` functions. Note that the Llama 3 8B is not hosted in your Virtual Warehouse since this is a Snowflake service. "
  },
  {
   "cell_type": "code",
   "id": "3845b170-400f-4839-b0fe-c9fbbfc35416",
   "metadata": {
    "language": "sql",
    "name": "llm_sentiment"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TEMP TABLE yelp_reviews_classified as\n(\nSELECT \n  review_id,\n  business_name,\n  review_date,\n  review_text,\n  SNOWFLAKE.CORTEX.AI_COMPLETE(\n    'llama3-8b',\n    CONCAT(\n      'You are a sentiment analysis assistant. ',\n      'Classify the following restaurant review as Positive, Negative, or Neutral. ',\n      'Only return one of those three words.\\n\\nReview: ',\n      review_text\n    )\n  ) AS llm_sentiment\nFROM yelp_reviews);\n\nSELECT * FROM yelp_reviews_classified;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46915d40-bc59-4ce7-b675-a70df0ad21ef",
   "metadata": {
    "language": "sql",
    "name": "reviews_analysis"
   },
   "outputs": [],
   "source": "SELECT business_name, llm_sentiment, COUNT(1) from yelp_reviews_classified\nGROUP BY ALL\nORDER BY business_name, llm_sentiment DESC ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "423da294-2fa6-44a0-b55a-9e050759f063",
   "metadata": {
    "name": "conclusion",
    "collapsed": false
   },
   "source": "Snowflake also offers direct sentiment analysis functions using cheaper, pre-trained ML models. The goal of this demo was to show the flexibility of this SQL API to directly analyze your data using LLMs without using external tools.\n\nThere are many other functions you can explore for your projects, which you can find here: https://docs.snowflake.com/user-guide/snowflake-cortex/aisql?lang=it%252f. Not all of them may be available using a Snowflake Free Trial but most of them should be available!"
  }
 ]
}